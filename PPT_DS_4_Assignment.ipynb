{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOCZ8uuZJApSVyNoxB8AdJI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kajal1301/Pwskills/blob/main/PPT_DS_4_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## General Linear Model:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8wLi8BsR7uEI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. What is the purpose of the General Linear Model (GLM)?\n"
      ],
      "metadata": {
        "id": "YZV9ddpw7zIl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* General linear model (GLM) is used to analyse and understand the relationship\n",
        "between a dependent variable (output feature) and one or more independent variables.\n",
        "* It is widely used in various felids such as regression analysis analysis of variance (ANOVA) and analysis of covariance (ANCOVA).\n",
        "* GLM assumes that dependent variable follows a normal distribution and the relationship between dependend and independent variable is linear.\n",
        "* GLM provides a framework for hypothesis testing, model comparison and estimation of parameters.\n",
        "\n"
      ],
      "metadata": {
        "id": "kYSb6vrgP3M3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. What are the key assumptions of the General Linear Model?\n"
      ],
      "metadata": {
        "id": "89q0qy4c7zF7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key assumptions of General Linear Model are:-\n",
        "1.\tLinearity : GLM assumes that the relationship between dependent variable and independent variables is linear.\n",
        "2.\tIndependence: This states that there should be no systematic relationship or dependency between the independent variables. If this condition violates, it can lead to biased and inefficient estimates.\n",
        "3.\tHomoscedasticity: Homoscedasticity assumes that variance of errors is constant across all independent variables.\n",
        "4.\tNormality: GLM assures that the errors or residuals follow normal distribution. This is necessary for hypothesis testing, confidence interval and model inderence.\n",
        "5.\tNo Multicollinearity: It refers to degree of correlation between independent variables. GLM assumes that independent variables are not correlated with each other.\n",
        "6.\tNo Endogeneity: GLM assumes that there is no correlation between the error term and one or more independent variables. Which if happens can lead to biased and inconsistent estimates.\n",
        "7.\tCorrect Specification: GLM assumes that model is correctly specified. Means the functional form of relationship between the variables is accurately represented in the model\n"
      ],
      "metadata": {
        "id": "pe8ROCv3QEbj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. How do you interpret the coefficients in a GLM?\n"
      ],
      "metadata": {
        "id": "Fmwm8dVj7zDy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cofficients in GLM depends on the specific model and variables used. In general coefficients provide information about the magnitude and direction of the effect that each independent variable has on the dependent variable, assuming all other variables in the model are held constant.\n",
        "1.\tcoefficient sign:  (+ or - ) sign indicates the direction of the relationship between the independent and dependent variable.\n",
        "2.\tMagnitude: magnitude reflects the size of effect that the independent variable has on the dependent variable, all else being equal.\n",
        "3.\tStatistical Significance: the statistical significance of a coefficient is determined by its p-value.\n",
        "4.\tAdjusted vs Unadjusted Coefficients: Model with independent variable might have adjusted coefficients sometimes. Adjusted coefficients provide a more accurate estimate of the relationship between a specific independent variable and dependent variable.\n"
      ],
      "metadata": {
        "id": "t0BhO8JfQK4i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. What is the difference between a univariate and multivariate GLM?\n",
        "\n"
      ],
      "metadata": {
        "id": "Gaa5a3pO7zA7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In **univariate GLM**, a single dependent variable is present having more than 1 independent variables. The goal of this model is to explain the variation of the sinhle dependent varuable with other independent variables.\n",
        "Example: linear regression, logistic regression\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "In **Multivariate GLM**, there are multiple dependent variables analyzed simultaneously by taking their relationships into account with their predictor variables.\n",
        "Example: multivariate regression, multivariate analysis of variance (MANOVA)\n"
      ],
      "metadata": {
        "id": "zVgocvawQPR3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Explain the concept of interaction effects in a GLM.\n"
      ],
      "metadata": {
        "id": "4xdl-Rd57y9_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In GLM, Interaction effects refer to the combined effect of two or more independent variables on the dependent variable that is greater (or lesser) than the sum of their individual effects. There are two kinds of interaction effects:\n",
        "1. Simple Interaction: A simple interaction occurs when one independent variable on the another independent variable.\n",
        "2. Complex Interaction: It occues whenone independent variable is dependent on two or more dependent variables."
      ],
      "metadata": {
        "id": "SloleOq2RjoB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. How do you handle categorical predictors in a GLM?\n"
      ],
      "metadata": {
        "id": "aoAwxmSM7y7a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In GLM categorical variables are handled via data encoding. Some of the data encoding techniques are:\n",
        "1.\t**One hot encoding**: In One Hot Encoding, each category is represented as a binary vector, where each bit corresponds to a unique category.\n",
        "Disadvantage of this type of encoding is that if we are having 100 categorical features, this will create 100 features which will increase dimensionality of data.\n",
        "Also it will create a sparse matrix.\n",
        "\n",
        "2. **Label Encoding**: It assigns a unique numerical label to each category in the variable.\n",
        "\tDisadvantage: it randomly assigns number as per alphabetical order and thus it might decrease the importance of a categorical variable.\n",
        "3. **Ordinal Encoding**: It helps assign ranks to the labels/ categorical data. In this technique each category is assigned a neumerical value based on its position in the order.\n",
        "4. **Target guided Ordinal Encoding**: this technique is used to encode each categorical variable based on their relationship with the target variable. This encoding is useful when we have a categorical variable with large number of unique categories. Here, we replace categorical variable with a numerical value based on the mean and median of the target variable for that category.\n",
        "\n"
      ],
      "metadata": {
        "id": "C5VFXwP0QzJx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. What is the purpose of the design matrix in a GLM?\n"
      ],
      "metadata": {
        "id": "V4ACURnV7y4f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Design matrix is a crucial component of GLM. It is a structured representation of independent variables in GLM organised in matrix format.\n",
        "The purpose of design matrix in GLM are:\n",
        "1. Encoding Independent Variables : The design matrix represents independent variables in a structured manner where each matrix corresponds to a specific independent variable, and each row corresponds to an observation or data point. It encodes the values of the independent variables for each observation, allowing GLM to incorporate them into the model.\n",
        "2. Incorporating Nonlinear Relationships:\n",
        "The design matrix can include  interactions of the original independent variables to analyse nonlinear relationships between the predictors and the dependent variable.\n",
        "\n",
        "3. Handling Categorical Variables:\n",
        "It is important to handle categorical variables in GLM as model only understands numerical value. This is done by data encoding.\n",
        "4. Estimating Coefficients:\n",
        "The design matrix allows the GLM to estimate the coefficients for each independent variable. By incorporating the design matrix into the GLM's estimation procedure, the model determines the relationship between the independent variables and the dependent variable, estimating the magnitude and significance of the effects of each predictor.\n",
        "\n",
        "5. Making Predictions:\n",
        "Once the GLM estimates the coefficients, the design matrix is used to make predictions for new, unseen data points. By multiplying the design matrix of the new data with the estimated coefficients, the GLM can generate predictions for the dependent variable based on the values of the independent variables.\n",
        "\n"
      ],
      "metadata": {
        "id": "VqIfpi-jTkbs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. How do you test the significance of predictors in a GLM?\n"
      ],
      "metadata": {
        "id": "GK78dUXe7y1y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can test significance of predictors in GLM by following ways:\n",
        "1. Hypothesis testing\n",
        "2. Chi square test\n",
        "3. ANOVA test\n",
        "4. By estimating coefficients\n"
      ],
      "metadata": {
        "id": "rGEUFD-8UxEk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. What is the difference between Type I, Type II, and Type III sums of squares in a GLM?\n"
      ],
      "metadata": {
        "id": "a2TqqsxS7yyq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **TYPE I sums of squares**:Type I Sums of Squares, or also called Sequential Sums of Squares, assign variation to the different variables in a sequential order.It calculates the reduction in the error sum of squares by adding each predictor variable sequentially in the order specified.\n",
        "\n",
        "* TYPE II Sums of square: Type II sums of squares assess the significance of each predictor variable in the model while adjusting for the effects of other variables already in the model. It calculates the reduction in the error sum of squares by adding each predictor variable, ignoring the order of variable entry.\n",
        "* **TYPE III Sums of square:** Type III sums of squares assess the significance of each predictor variable in the model, taking into account the effects of all other variables in the model. It calculates the reduction in the error sum of squares by adding each predictor variable, adjusting for the effects of all other variables."
      ],
      "metadata": {
        "id": "stEw-dcwVowJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10. Explain the concept of deviance in a GLM."
      ],
      "metadata": {
        "id": "M9R-hDwF7yv7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a GLM, deviance is a measure of goodness of fit of the model. It quantifies the discrepency between the observed data and the predictions made by the model.\n",
        "The deviance is calculated as follows:\n",
        "\n",
        "Deviance = -2 * (log-likelihood of fitted model - log-likelihood of saturated model)"
      ],
      "metadata": {
        "id": "-kux5bD07ytX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regression:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Vi4iVS0y7yqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What is regression analysis and what is its purpose?\n"
      ],
      "metadata": {
        "id": "XEqhiYrI7ynL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regression is a kind of Supervised Machine Learning which is used when the dependent feature is having continuous value.\n",
        "\n",
        "For example : heights of students in the class.\n",
        "The purpose of regression analysis is to find the relationship between independent and dependent features where the dependent (output) variable is having continuous values."
      ],
      "metadata": {
        "id": "GmGAhZ3PX05g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What is the difference between simple linear regression and multiple linear regression?\n"
      ],
      "metadata": {
        "id": "S4OJmI2pYUKj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In **simple linear Regression**, there is only 1 independent variable which is used to estimate or predict the dependent variable. Here we can assume that the dependent variable is having a linear relationship witht he independent variable. The goal here is to find the best fit line that minimizes the difference between the observed values and predicted values based on linear relationship.\n",
        "\n",
        "In Multiple Linear Regression, There are two or more than 3 independent variables that are used to estimate or predict the dependent variable. The goal here is to find the best fit line or hyperplane that minimizes the relationship between multiple independent variables and the dependent variable."
      ],
      "metadata": {
        "id": "1IoZ0RH1YUHs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 13. How do you interpret the R-squared value in regression?"
      ],
      "metadata": {
        "id": "pkQXBoltYUFU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "R-squared is also known as coefficient of determination. It is a statistical measure that represents the proportion of variance in the dependent variable that is explained by the independent variables in a regression model.\n",
        " * R-squared= 1 - SS_res/SS_total\n",
        "* where SS_res= sum of squares of residuals\n",
        "* SS_total= sum of squares of total\n",
        " * It ranges between 0 and 1.\n",
        " * If R-square value reaches towards 1, then this is the case of overfitting.\n",
        ""
      ],
      "metadata": {
        "id": "i1AxfgtpZeRC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 14. What is the difference between correlation and regression?\n"
      ],
      "metadata": {
        "id": "j78kSlZFZeNa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Correlation** is a satistical measure that establishes the relationships between two variables.\n",
        "It quantifies how closely the values of two variables are related to each other. Correlation coefficients range from -1 to +1, where -1 indicates a perfect negative correlation, +1 indicates a perfect positive correlation, and 0 indicates no correlation.\n",
        "* **Regression**, on the other hand, is a statistical technique used to model the relationship between a dependent variable and one or more independent variables.\n",
        "Regression analysis can be used to understand the impact of specific variables on the outcome and make predictions or infer causal relationships."
      ],
      "metadata": {
        "id": "Pn2artNzZeKP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 15. What is the difference between the coefficients and the intercept in regression?"
      ],
      "metadata": {
        "id": "W79MsJ0oZeFc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Intercept:**\n",
        "The intercept, often denoted as \"b₀\" or \"β₀,\" is the value of the dependent variable when all independent variables in the model are set to zero. It represents the baseline or starting point of the regression line or surface.\n",
        "* Coefficients:\n",
        "The coefficients, also known as regression coefficients, they capture the relationship between the dependent variable and each independent variable. They indicate the direction (positive or negative) and magnitude of the effect that each independent variable has on the dependent variable.\n"
      ],
      "metadata": {
        "id": "CxiQaKj9ZeCl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 16. How do you handle outliers in regression analysis?"
      ],
      "metadata": {
        "id": "6VSFt0ECZd_0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In regression, if there are outliers then it squared error will become too high, thus effecting the performance of the model.\n",
        "Thus outliers can be handled using multiple ways:-\n",
        "1. If the points are too far, then they can be reduced.\n",
        "2. Variables can be transformed /standardized/ Normalized to reduce the outliers.\n",
        "3. In case of polynomial regression, we can take small clusters of data points to check the behaviour of outliers.\n",
        "\n"
      ],
      "metadata": {
        "id": "rJRBj1TfYUCx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 17. What is the difference between ridge regression and ordinary least squares regression?"
      ],
      "metadata": {
        "id": "vQGZF3HgwdIb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* One of the disadvantages of using Linear regression (Ordinary least squares) is overfitting.\n",
        "*  So it is essential to find solutions that help provide more regularization. One of the solutions is Ridge Regression.\n",
        "* Ridge regression is a linear model for regression like Linear regression (Ordinary least squares).\n",
        "*  But there is a difference that helps make the Ridge regression more regularized and thus avoid the problem of overfitting.\n",
        "* The ordinary least squares model seeks to find the coefficients that minimize the mean squared error.\n",
        "* On the other hand, Ridge Regression tries to find the coefficients that minimize the mean squared error and wants the magnitude of coefficients to be as small as possible. That means each feature should have a little effect on the outcome.\n",
        "* Therefore, Ridge Regression will perform worse than the ordinary least squares model on the training set. But it will give us better regularization and performance on the test set.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BBUG0luzv1AO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 18. What is heteroscedasticity in regression and how does it affect the model?"
      ],
      "metadata": {
        "id": "_3ncjJxwv08_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Heteroscedasticity in regression refers to a situation where the variability of the errors (residuals) in a regression model is not constant across different values of dependent variable. In other words, the spread of the residuals varies as the values of the independent variables change. This violates one of the assumptions of classical linear regression, known as homoscedasticity or constant variance of errors.\n",
        "It affects the model by:\n",
        "1. Giving Inaccurate Predictions,\n",
        "2. Inefficient standard errors\n",
        "3. Estimating Biased Coefficients"
      ],
      "metadata": {
        "id": "7CIg12_nv05e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 19.How do you handle multicollinearity in regression analysis?"
      ],
      "metadata": {
        "id": "vk8jAuVHxL2R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Muticollinearty in regression can be handled by:-\n",
        "1. Identifying the collinearity of features.\n",
        "2. Removing one of the correlated features.\n",
        "3. Feature Selection\n",
        "4. Reducing Dimensions of the data with the help of dimensionality reduction techniques such as PCA, Tsne etc.\n",
        "5. Feature Standardization and Feature Normalization"
      ],
      "metadata": {
        "id": "NRJ3GONRxLww"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 20. What is polynomial regression and when is it used?"
      ],
      "metadata": {
        "id": "03C-0dRDYT_2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Polynomial Regression is an extension of linear Regression that models the relationship between the independent variables and the dependent variable as a higher degree polynomial function.\n",
        "* It allows capturing non- linear relationship between the variables.\n",
        "* Polynomial regression is used in situations where the relationship between the variables is not linear and cannot be adequately represented by a straight line.\n",
        "* It is particularly useful when there is prior knowledge or a theoretical basis to believe that the relationship between the variables should be nonlinear, or when examining data patterns suggests a curvilinear relationship.\n"
      ],
      "metadata": {
        "id": "hfSDPom1YT9o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss function:"
      ],
      "metadata": {
        "id": "hvz82lS5y_XP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 21. What is a loss function and what is its purpose in machine learning?"
      ],
      "metadata": {
        "id": "nu2uq5jgy_Sf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Loss function, in machine learning is also known as error function or cost function.\n",
        "* It is a mathematical measure that quantifies the discrepency between the predicted and observed values of dependent variable.\n",
        "*  The purpose of loss function is to provide a measure of how well the model is performiing to help the learning process to minimize the error between observed values and actual values.\n"
      ],
      "metadata": {
        "id": "Bgksvl5d0tn5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 22. What is the difference between a convex and non-convex loss function?"
      ],
      "metadata": {
        "id": "RMe0Rwi3y_Pn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Convex Loss Function:**\n",
        "A convex loss function is one in which the combination of any two points on the loss function curve lies above or on the curve itself.\n",
        "\n",
        " Mathematically, a function f(x) is convex if, for any two points x₁ and x₂ in its domain and any value λ between 0 and 1, the following condition holds:\n",
        "\n",
        "f(λx₁ + (1 - λ)x₂) ≤ λf(x₁) + (1 - λ)f(x₂)\n",
        "\n",
        "In simpler terms, if you draw a straight line connecting any two points on the curve, the line will always lie above the curve.\n",
        "* **Non-Convex Loss Function:**\n",
        "A non-convex loss function does not satisfy the convexity property. In other words, there exist points on the loss function curve for which the line connecting two of those points dips below the curve.\n",
        "\n",
        "Non-convex loss functions often have multiple local minima, making it more challenging to find the optimal solution. Optimization algorithms may converge to a local minimum instead of the global minimum, resulting in a suboptimal solution."
      ],
      "metadata": {
        "id": "TeSrLOuo1Pm_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 23. What is mean squared error (MSE) and how is it calculated?"
      ],
      "metadata": {
        "id": "N2DxbPUFzzKk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mean Squared Error is a loss function used in regression problems to measure the average squared difference between the predicted values and the true values of the target variable. It quantifies the overall goodness of fit of the regression model.\n",
        "* Its advantages are:-\n",
        "It is differentiable and also has only 1 local and 1 global minima.\n",
        "* Its disadvantage is that MSE is not Robust to outliers. Also squaring the loss will square the unit of the variable as well.\n",
        "\n",
        "Mathematically, the MSE can be represented as:\n",
        "\n",
        "MSE = SSE / n\n",
        "\n",
        "where SSE is the sum of squared errors and n is the number of data points."
      ],
      "metadata": {
        "id": "UYntF8XV1lMD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 24. What is mean absolute error (MAE) and how is it calculated?"
      ],
      "metadata": {
        "id": "cWHNSNKpz05C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mean Absolute Error (MAE) is a loss function in regression analysis used to measure the average absolute difference between the predicted values and the true values of the target variable. It provides a straightforward measure of the model's average prediction error.\n",
        "* Its advantages are: It is Robust to outliers. Also the error has same unit as dependent variable.\n",
        "* Disadvantage: Convergence usually takes time and its optimization is complex.\n",
        "\n",
        "Mathematically, the MAE can be represented as:\n",
        "\n",
        "MAE = SAE / n\n",
        "\n",
        "where SAE is the sum of absolute errors and n is the number of data points."
      ],
      "metadata": {
        "id": "mRCvpKvt14Hx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 25. What is log loss (cross-entropy loss) and how is it calculated?"
      ],
      "metadata": {
        "id": "P6GTKNQJz2js"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Log loss, also known as cross-entropy loss or logarithmic loss, is a loss function used in classification tasks, especially in binary classification or multi-class classification with probabilistic models.\n",
        "\n",
        "It measures the discrepancy between predicted class probabilities and the true class labels.\n",
        "\n",
        "Mathematically, the log loss (cross-entropy loss) can be represented as:\n",
        "\n",
        "Log Loss = (-1/n) * ∑[y * log(p) + (1 - y) * log(1 - p)]\n",
        "\n",
        "where y is the true class label (0 or 1), p is the predicted probability of the true class, and the summation is performed over all data points."
      ],
      "metadata": {
        "id": "WuQnP26C2zDY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 26. How do you choose the appropriate loss function for a given problem?"
      ],
      "metadata": {
        "id": "tZ7PfoRSz4JW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Depending upon the type of problem statement and model, a suitable loss function needs to be selected from the set of available. Different parameters like type of machine learning algorithm, degrees of the percentage of outliers in the provided dataset, ease of calculating derivatives etc. play their role in choosing loss function.\n",
        "\n",
        "* Regression Loss Functions\n",
        "  1. Mean Squared Error Loss\n",
        "  2. Mean Squared Logarithmic Error Loss\n",
        "  3. Mean Absolute Error Loss\n",
        "* Binary Classification Loss Functions\n",
        "  1. Binary Cross-Entropy\n",
        "  2. Hinge Loss\n",
        "  3. Squared Hinge Loss\n",
        "* Multi-Class Classification Loss Functions\n",
        "  1. Multi-Class Cross-Entropy Loss\n",
        "  2. Sparse Multiclass Cross-Entropy Loss\n",
        "  3. Kullback Leibler Divergence Loss\n"
      ],
      "metadata": {
        "id": "86GFJ5nF2-RB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 27. Explain the concept of regularization in the context of loss functions."
      ],
      "metadata": {
        "id": "X4-r3hzhz6Cz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Regularization is a technique used to prevent the model from overfitting by adding some extra information into it.\n",
        "* It is commonly used in machine learning and statistical modelling to prevent overfitting and improve the generalization ability of the model.\n",
        "* The general form of regularized loss function is = Loss + Regularization Term.\n"
      ],
      "metadata": {
        "id": "vN_3FlEw0CXS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 28. What is Huber loss and how does it handle outliers?"
      ],
      "metadata": {
        "id": "j1AZpJR5z9UV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Huber loss is a loss function that is commonly used in regression problems to balance the effects of outliers and inliers in the data.\n",
        "\n",
        "Unlike the mean squared error (MSE) loss, which treats all errors equally, the Huber loss gives less weight to large errors, making it more robust to outliers.\n",
        "\n",
        "The Huber loss function is defined as follows:\n",
        "\n",
        "Huber Loss = { 0.5 * (y - ŷ)^2 if |y - ŷ| ≤ δ\n",
        "{ δ * |y - ŷ| - 0.5 * δ^2 if |y - ŷ| > δ\n",
        "\n",
        "where y is the true value, ŷ is the predicted value, and δ is a parameter that determines the threshold for the transition between the quadratic and linear regions of the loss function."
      ],
      "metadata": {
        "id": "Ibed_wjY4JjC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 29. What is quantile loss and when is it used?"
      ],
      "metadata": {
        "id": "NzIyl01Oz_GV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantile loss, also known as pinball loss, is a loss function used in quantile regression to measure the deviation between predicted quantiles and the true quantiles of a target variable.\n",
        "\n",
        "It is particularly useful when the goal is to estimate conditional quantiles of the response variable rather than the conditional mean.\n",
        "\n",
        "The quantile loss is defined as follows:\n",
        "\n",
        "Quantile Loss = ∑[ρ(y - ŷ)],\n",
        "\n",
        "where ρ is a function that determines the shape of the loss function, y is the true value, and ŷ is the predicted value."
      ],
      "metadata": {
        "id": "HoTriR2D4wzP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 30. What is the difference between squared loss and absolute loss?\n"
      ],
      "metadata": {
        "id": "nm8E_mjpYT62"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Key difference between Squared loss and absolute loss is:\n",
        "1. Squared Loss is not Robust to outliers whereas Absolute Loss is Robust to outliers.\n",
        "2. Squared loss value has the unit which is squared to that of the dependent variable whereas the Absolute loss value has the same unit as compared to that of dependent variable values.\n",
        "3. Squared Loss function is differentiable, we can get local and global minima using squared loss function but there is convergence in absolute loss function.\n",
        "4. In absolute loss function, optimization is complex."
      ],
      "metadata": {
        "id": "VZN20zP9YT4F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimizer (GD):"
      ],
      "metadata": {
        "id": "AW1zgkVKYT1M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 31. What is an optimizer and what is its purpose in machine learning?"
      ],
      "metadata": {
        "id": "s1-JREUlYTxo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimizer refers to an algorithm or method used to adjust the parameters of a model to minimize the loss function and improve the model's performance.\n",
        "\n",
        "The optimizer plays a crucial role in the training process by iteratively updating the model's parameters based on the gradients of the loss function.\n",
        "\n",
        "The purpose of an optimizer in machine learning is to find the optimal set of parameter values that minimize the discrepancy between the predicted values and the true values of the target variable. It aims to improve the model's ability to generalize and make accurate predictions on unseen data."
      ],
      "metadata": {
        "id": "K1e1ejEp6HZ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 32. What is Gradient Descent (GD) and how does it work?"
      ],
      "metadata": {
        "id": "IaGqzg2T6Qdv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient Descent (GD) is an optimization algorithm used to iteratively update the parameters of a model in order to minimize a given loss function. It is widely used in machine learning and deep learning for parameter estimation and model training.\n"
      ],
      "metadata": {
        "id": "h8eDDhqS6TKc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 33. What are the different variations of Gradient Descent?"
      ],
      "metadata": {
        "id": "5cRQN4Km6THn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Batch Gradient Descent (BGD) :Batch Gradient Descent calculates the gradient of the loss function using the entire training dataset in each iteration. It performs a parameter update based on the average gradient across all the training examples.\n",
        "2. Stochastic Gradient Descent (SGD):Stochastic Gradient Descent calculates the gradient using only one randomly selected data point (or a single data point from a mini-batch) in each iteration. It performs a parameter update based on the gradient of that single data point. SGD is computationally efficient as it requires the computation of the gradient for one data point at a time.\n",
        "3. Mini-Batch Gradient Descent:Mini-Batch Gradient Descent is a compromise between BGD and SGD. It calculates the gradient using a small random subset, or mini-batch, of the training data in each iteration.\n",
        "4. Momentum-Based Gradient Descent:Momentum-Based Gradient Descent algorithms introduce a momentum term that helps accelerate convergence and enhance stability."
      ],
      "metadata": {
        "id": "JKW9wt_X_GSU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 34. What is the learning rate in GD and how do you choose an appropriate value?"
      ],
      "metadata": {
        "id": "VaTKGDwr6TCK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Learning rate is a hyperparameter that controls the step size taken in each iteration during the parameter update process.  \n",
        "\n",
        "The learning rate gives you control of how big (or small) the updates are going to be. A bigger learning rate means bigger updates and, hopefully, a model that learns faster.\n",
        "It determines the magnitude of the parameter updates based on the gradient of the loss function.\n",
        "\n",
        "Choosing an appropriate learning rate is crucial for effective training and convergence.\n",
        "We can choose a learning rate by using difference values of learning rate and perform Cross validation and grid search and check the results."
      ],
      "metadata": {
        "id": "GcJEAEaX-SWz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 35. How does GD handle local optima in optimization problems?"
      ],
      "metadata": {
        "id": "iObLwJ7E6S_o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GD can handle local optima in optimization problems by:-\n",
        "1. Using regularization techniques\n",
        "2. adding noise\n",
        "3. Choosing different learning rates\n",
        "4. By choosing multiple starting points"
      ],
      "metadata": {
        "id": "7Qm6GYc291xc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 36. What is Stochastic Gradient Descent (SGD) and how does it differ from GD?"
      ],
      "metadata": {
        "id": "UQvpMp8g6S8h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stochastic Gradient Descent (SGD) is a variant of the Gradient Descent (GD) optimization algorithm used to train machine learning models.\n",
        "* It differs from GD in how it updates the model's parameters by considering only a single training example (or a small subset called a mini-batch) in each iteration, rather than the entire training dataset.\n",
        "* The main difference between stochastic gradient descent (SGD) and gradient descent (GD) is the number of data points used before each update of the parameters.\n",
        "* GD spans over the entire dataset once before each update, whereas SGD randomly takes just one data point for each update12.\n",
        "* In GD, you have to run through all the samples in your training set to do a single update for a parameter in a particular iteration, whereas in SGD, you use only one or a subset of training samples from your training set to do the update for a parameter in a particular iteration2."
      ],
      "metadata": {
        "id": "UudKEvXw9g-E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 37. Explain the concept of batch size in GD and its impact on training."
      ],
      "metadata": {
        "id": "eS-f-LRj6wQ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Gradient Descent, the batch size refers to the number of training examples used to compute the gradient and update the model's parameters in each iteration.\n",
        "* A larger batch size implies that more training examples are used to compute the gradient in each iteration. This can result in more accurate gradient estimates due to a larger sample size. However, it also requires more memory and computational resources to process the larger batch, which can slow down the training process.\n",
        "Larger batch sizes often lead to more stable updates and smoother convergence.\n",
        "\n",
        "* A smaller batch size means that fewer training examples are used in each iteration. This reduces memory requirements and speeds up computations. However, the gradient estimates may be noisier due to a smaller sample size, resulting in more variance during training."
      ],
      "metadata": {
        "id": "OzVdgGxK9Gm9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 38. What is the role of momentum in optimization algorithms?"
      ],
      "metadata": {
        "id": "NisD9liq6S54"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Momemtum in GD is used to accelerate the convergence and enhance the stability of the learning process.\n",
        "* In the Momentum-based Gradient Optimizer, a fraction of the previous update is added to the current update, which creates a momentum effect that helps the algorithm to move faster towards the minimum.\n",
        "*  Momentum enables faster convergence by adding a momentum term that accumulates the gradients over time. It increases the update step sizes for dimensions that consistently show a significant gradient, allowing the optimizer to quickly move along steep slopes towards the minimum.\n",
        "*  Higher momentum can allow for larger effective learning rates, enabling faster convergence, while lower momentum may require smaller learning rates to ensure stability."
      ],
      "metadata": {
        "id": "sRRNZpJI8hZy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 39. What is the difference between batch GD, mini-batch GD, and SGD?"
      ],
      "metadata": {
        "id": "Xj6X1fr76S27"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Batch Gradient Descent (BGD):\n",
        "* BGD calculates the gradient of the loss function using the entire training dataset in each iteration.\n",
        "* It performs a parameter update based on the average gradient across all the training examples.\n",
        "* BGD provides a precise estimate of the gradient but can be computationally expensive for large datasets.\n",
        "* BGD takes more time to converge, but the updates are more stable and less noisy compared to other variants.\n",
        "2. Mini-Batch Gradient Descent:\n",
        "* Mini-Batch Gradient Descent calculates the gradient using a randomly selected subset, or mini-batch, of the training data in each iteration.\n",
        "* The mini-batch typically contains a small number of data points, such as 10 to 1000, chosen randomly from the training set.\n",
        "* It strikes a balance between the accuracy of BGD and the computational efficiency of SGD.\n",
        "* Mini-Batch GD provides a more accurate estimate of the gradient compared to SGD due to the use of multiple data points in each iteration.\n",
        "* It is commonly used in practice as it allows for parallelization and efficient computation, especially when dealing with large datasets.\n",
        "3. Stochastic Gradient Descent (SGD):\n",
        "* SGD calculates the gradient using only one randomly selected data point (or a single data point from a mini-batch) in each iteration.\n",
        "* It performs a parameter update based on the gradient of that single data point.\n",
        "* SGD is computationally efficient and faster per iteration compared to BGD and Mini-Batch GD since it only requires the computation of the gradient for one data point at a time.\n",
        "* SGD exhibits more stochastic behavior and can be noisy during training due to the high variance in the estimated gradients.\n",
        "* Although SGD has a higher likelihood of converging to a local minimum, it can escape shallow local minima more easily compared to BGD and Mini-Batch GD."
      ],
      "metadata": {
        "id": "uxzFK9VG8N7f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 40. How does the learning rate affect the convergence of GD?"
      ],
      "metadata": {
        "id": "ezejQbNQ6Sz5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. If Learning rate is set to a large value: It can lead to divergence, where loss may increase instead of decreasing. The algorithm might fail to converge or may converge very slowly.\n",
        "2. If Learning rate is too small, the algorithm will take smaller steps, which can result in slow convergence. It may take a large number of iterations for GD to reach the minimum of the loss function.\n",
        "3. If Learning Rate is optimal, then it ccan make a balence between convergence speed and stability which will allow GD to make progress towards minimum without overshooting or converging too slowly. . It can be determined through experimentation or by using techniques like learning rate schedules or adaptive learning rate methods."
      ],
      "metadata": {
        "id": "MNQGrEPE7TAo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ".\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "I5uSBtc8YTtG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regularization:"
      ],
      "metadata": {
        "id": "ay0CkkRQ65kv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 41. What is regularization and why is it used in machine learning?"
      ],
      "metadata": {
        "id": "Bj7I-1NeBLhE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regularization is a technique used to prevent overfitting and improve the generalization performance of a model. Overfitting occurs when a model learns to fit the training data very well but fails to generalize well to unseen data.\n",
        "It introduces a penalty term  to the model's objective function, which discourages it from learning complex or intricate patterns in the training data that might not generalize well."
      ],
      "metadata": {
        "id": "6aCWhEVkBLdN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 42. What is the difference between L1 and L2 regularization?"
      ],
      "metadata": {
        "id": "AzZSL9NABuIL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* L1 regularization adds a penalty proportional to the sum of the absolute values of the model's coefficients. This encourages sparsity in the model, meaning it encourages some of the coefficients to be exactly zero, effectively performing feature selection and eliminating less important features.\n",
        "\n",
        "* L2 regularization adds a penalty proportional to the sum of the squared values of the model's coefficients. This encourages the model's coefficients to be small but non-zero, distributing the importance of features more evenly."
      ],
      "metadata": {
        "id": "2NVPkBSDBuF4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 43. Explain the concept of ridge regression and its role in regularization."
      ],
      "metadata": {
        "id": "feA1ToiQBuC-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ridge regression is a linear regression technique that incorporates L2 regularization to mitigate the issues of overfitting and improve the generalization performance of the model. It is an extension of ordinary least squares (OLS) regression.\n",
        "\n",
        "The role of ridge regression in regularization is to strike a balance between fitting the training data well and keeping the model's coefficients small. The penalty term encourages the model to find a solution where the coefficient values are spread out more evenly and prevents them from becoming too large, reducing the complexity of the model."
      ],
      "metadata": {
        "id": "Fp7cYMafBLat"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 44. What is the elastic net regularization and how does it combine L1 and L2 penalties?"
      ],
      "metadata": {
        "id": "NWRQ6ZblBLYT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Elastic Net regularization is a technique that combines both L1 (Lasso) and L2 (Ridge) regularization penalties in a linear regression model. It is designed to address the limitations of using either L1 or L2 regularization alone.\n",
        "\n",
        "In elastic net regularization, the objective function includes both the L1 and L2 penalty terms. The objective function can be represented as:\n",
        "\n",
        "Objective = Sum of squared residuals + λ₁ * (Sum of squared coefficients) + λ₂ * (Sum of absolute values of coefficients)\n",
        "\n",
        "The L1 penalty encourages sparsity in the model, promoting some coefficients to be exactly zero, thereby performing feature selection. On the other hand, the L2 penalty encourages smaller but non-zero coefficients, distributing the importance of features more evenly."
      ],
      "metadata": {
        "id": "bDS7rHnRBLVk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 45. How does regularization help prevent overfitting in machine learning models?"
      ],
      "metadata": {
        "id": "TicLP2crBLSj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Regularization helps prevent overfitting in machine learning models through the introduction of a penalty term in the model's objective function. By incorporating this penalty term, regularization discourages the model from learning overly complex or intricate patterns in the training data that may not generalize well to unseen data."
      ],
      "metadata": {
        "id": "uFy3q4C7BLPT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 46. What is early stopping and how does it relate to regularization?"
      ],
      "metadata": {
        "id": "g8dZRYzaCYGd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Early stopping is a technique used in machine learning to prevent overfitting by monitoring the performance of a model during training and stopping the training process when the model's performance on a validation set starts to deteriorate.\n",
        "\n",
        "Early stopping relates to regularization in the sense that it helps control the complexity of the model and prevents it from overfitting.\n",
        "\n",
        " Regularization techniques like L1 or L2 regularization introduce penalties to the objective function, encouraging the model to generalize better. However, these penalties alone may not be sufficient to determine the optimal point at which to stop training. Early stopping complements regularization by providing a mechanism to determine when to halt training based on the observed performance on a validation set."
      ],
      "metadata": {
        "id": "T5I3A43xCYEX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 47. Explain the concept of dropout regularization in neural networks."
      ],
      "metadata": {
        "id": "khPj3JJbCYCE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Dropout regularization is a technique used in neural networks to prevent overfitting and improve the generalization performance of the model. It involves randomly disabling or \"dropping out\" a fraction of the neurons during training.\n",
        "\n",
        "The key idea behind dropout is to introduce noise or randomness into the network by temporarily removing a subset of neurons along with their connections during each training iteration. This prevents the network from relying too heavily on any particular set of neurons and encourages the network to learn more robust and distributed representations."
      ],
      "metadata": {
        "id": "Lf8w3WmrCX-x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 48. How do you choose the regularization parameter in a model?"
      ],
      "metadata": {
        "id": "9YG4nVmBCylW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can choose the right regularization paramtere by:\n",
        "1. Grid Search\n",
        "2. Random Search\n",
        "3. Cross Validation"
      ],
      "metadata": {
        "id": "npttL1C1Cyim"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 49. What is the difference between feature selection and regularization?"
      ],
      "metadata": {
        "id": "83ePK6qvCyfj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature selection and regularization are two distinct techniques used in machine learning to improve model performance and address the issue of overfitting.\n",
        "\n",
        "\n",
        "Feature selection refers to the process of identifying and selecting a subset of relevant features from the original set of available features.\n",
        "\n",
        "Regularization, on the other hand, is a technique used during the training process of a model to prevent overfitting and improve generalization.\n",
        "\n",
        "Feature selection focuses on choosing a subset of relevant features, while regularization aims to control the complexity of the model during training.\n"
      ],
      "metadata": {
        "id": "tVRWS2ZVDDJp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 50. What is the trade-off between bias and variance in regularized models?\n"
      ],
      "metadata": {
        "id": "1tDlREAx6-mV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bias Variance Tradeoff refers to the final goal of our ML model which is to get a low generalization error.\n",
        "i.e. Low Bias and Low Variance.\n",
        "\n",
        "Using regularization , Increasing the regularization parameter (λ) leads to an increase in bias and a decrease in variance.\n",
        "\n",
        "Decreasing the regularization parameter (λ) leads to a decrease in bias and an increase in variance.\n",
        "\n",
        "The optimal regularization parameter strikes a balance between bias and variance to achieve the best trade-off.\n",
        "To acheive this techniques such as Grid Search or Cross Validation are used.\n"
      ],
      "metadata": {
        "id": "g529c9mEDSSA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM:\n",
        "\n",
        "51. What is Support Vector Machines (SVM) and how does it work?\n",
        "52. How does the kernel trick work in SVM?\n",
        "53. What are support vectors in SVM and why are they important?\n",
        "54. Explain the concept of the margin in SVM and its impact on model performance.\n",
        "55. How do you handle unbalanced datasets in SVM?\n",
        "56. What is the difference between linear SVM and non-linear SVM?\n",
        "57. What is the role of C-parameter in SVM and how does it affect the decision boundary?\n",
        "58. Explain the concept of slack variables in SVM.\n",
        "59. What is the difference between hard margin and soft margin in SVM?\n",
        "60. How do you interpret the coefficients in an SVM model?\n"
      ],
      "metadata": {
        "id": "6W2loIKe68z9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision Trees:\n",
        "\n",
        "61. What is a decision tree and how does it work?\n",
        "62. How do you make splits in a decision tree?\n",
        "63. What are impurity measures (e.g., Gini index, entropy) and how are they used in decision trees?\n",
        "64. Explain the concept of information gain in decision trees.\n",
        "65. How do you handle missing values in decision trees?\n",
        "66. What is pruning in decision trees and why is it important?\n",
        "67. What is the difference between a classification tree and a regression tree?\n",
        "68. How do you interpret the decision boundaries in a decision tree?\n",
        "69. What is the role of feature importance in decision trees?\n",
        "70. What are ensemble techniques and how are they related to decision trees?\n"
      ],
      "metadata": {
        "id": "VtgKY6P57Ab8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensemble Techniques:\n",
        "\n",
        "71. What are ensemble techniques in machine learning?\n",
        "72. What is bagging and how is it used in ensemble learning?\n",
        "73. Explain the concept of bootstrapping in bagging.\n",
        "74. What is boosting and how does it work?\n",
        "75. What is the difference between AdaBoost and Gradient Boosting?\n",
        "76. What is the purpose of random forests in ensemble learning?\n",
        "77. How do random forests handle feature importance?\n",
        "78. What is stacking in ensemble learning and how does it work?\n",
        "79. What are the advantages and disadvantages of ensemble techniques?\n",
        "80. How do you choose the optimal number of models in an ensemble?\n",
        "\n"
      ],
      "metadata": {
        "id": "BYnXM6dV7AY-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "aerhD7ob7AV-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JEd3SvkA7ASV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4uyk_DYV7AN5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IJX-N8qJ69N2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}