{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNYbeFj7JpoyPmyd2tHKR3E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kajal1301/Pwskills/blob/main/PPT_DS_4_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## General Linear Model:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8wLi8BsR7uEI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. What is the purpose of the General Linear Model (GLM)?\n"
      ],
      "metadata": {
        "id": "YZV9ddpw7zIl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* General linear model (GLM) is used to analyse and understand the relationship\n",
        "between a dependent variable (output feature) and one or more independent variables.\n",
        "* It is widely used in various felids such as regression analysis analysis of variance (ANOVA) and analysis of covariance (ANCOVA).\n",
        "* GLM assumes that dependent variable follows a normal distribution and the relationship between dependend and independent variable is linear.\n",
        "* GLM provides a framework for hypothesis testing, model comparison and estimation of parameters.\n",
        "\n"
      ],
      "metadata": {
        "id": "kYSb6vrgP3M3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. What are the key assumptions of the General Linear Model?\n"
      ],
      "metadata": {
        "id": "89q0qy4c7zF7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key assumptions of General Linear Model are:-\n",
        "1.\tLinearity : GLM assumes that the relationship between dependent variable and independent variables is linear.\n",
        "2.\tIndependence: This states that there should be no systematic relationship or dependency between the independent variables. If this condition violates, it can lead to biased and inefficient estimates.\n",
        "3.\tHomoscedasticity: Homoscedasticity assumes that variance of errors is constant across all independent variables.\n",
        "4.\tNormality: GLM assures that the errors or residuals follow normal distribution. This is necessary for hypothesis testing, confidence interval and model inderence.\n",
        "5.\tNo Multicollinearity: It refers to degree of correlation between independent variables. GLM assumes that independent variables are not correlated with each other.\n",
        "6.\tNo Endogeneity: GLM assumes that there is no correlation between the error term and one or more independent variables. Which if happens can lead to biased and inconsistent estimates.\n",
        "7.\tCorrect Specification: GLM assumes that model is correctly specified. Means the functional form of relationship between the variables is accurately represented in the model\n"
      ],
      "metadata": {
        "id": "pe8ROCv3QEbj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. How do you interpret the coefficients in a GLM?\n"
      ],
      "metadata": {
        "id": "Fmwm8dVj7zDy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cofficients in GLM depends on the specific model and variables used. In general coefficients provide information about the magnitude and direction of the effect that each independent variable has on the dependent variable, assuming all other variables in the model are held constant.\n",
        "1.\tcoefficient sign:  (+ or - ) sign indicates the direction of the relationship between the independent and dependent variable.\n",
        "2.\tMagnitude: magnitude reflects the size of effect that the independent variable has on the dependent variable, all else being equal.\n",
        "3.\tStatistical Significance: the statistical significance of a coefficient is determined by its p-value.\n",
        "4.\tAdjusted vs Unadjusted Coefficients: Model with independent variable might have adjusted coefficients sometimes. Adjusted coefficients provide a more accurate estimate of the relationship between a specific independent variable and dependent variable.\n"
      ],
      "metadata": {
        "id": "t0BhO8JfQK4i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. What is the difference between a univariate and multivariate GLM?\n",
        "\n"
      ],
      "metadata": {
        "id": "Gaa5a3pO7zA7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In **univariate GLM**, a single dependent variable is present having more than 1 independent variables. The goal of this model is to explain the variation of the sinhle dependent varuable with other independent variables.\n",
        "Example: linear regression, logistic regression\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "In **Multivariate GLM**, there are multiple dependent variables analyzed simultaneously by taking their relationships into account with their predictor variables.\n",
        "Example: multivariate regression, multivariate analysis of variance (MANOVA)\n"
      ],
      "metadata": {
        "id": "zVgocvawQPR3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Explain the concept of interaction effects in a GLM.\n"
      ],
      "metadata": {
        "id": "4xdl-Rd57y9_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In GLM, Interaction effects refer to the combined effect of two or more independent variables on the dependent variable that is greater (or lesser) than the sum of their individual effects. There are two kinds of interaction effects:\n",
        "1. Simple Interaction: A simple interaction occurs when one independent variable on the another independent variable.\n",
        "2. Complex Interaction: It occues whenone independent variable is dependent on two or more dependent variables."
      ],
      "metadata": {
        "id": "SloleOq2RjoB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. How do you handle categorical predictors in a GLM?\n"
      ],
      "metadata": {
        "id": "aoAwxmSM7y7a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In GLM categorical variables are handled via data encoding. Some of the data encoding techniques are:\n",
        "1.\t**One hot encoding**: In One Hot Encoding, each category is represented as a binary vector, where each bit corresponds to a unique category.\n",
        "Disadvantage of this type of encoding is that if we are having 100 categorical features, this will create 100 features which will increase dimensionality of data.\n",
        "Also it will create a sparse matrix.\n",
        "\n",
        "2. **Label Encoding**: It assigns a unique numerical label to each category in the variable.\n",
        "\tDisadvantage: it randomly assigns number as per alphabetical order and thus it might decrease the importance of a categorical variable.\n",
        "3. **Ordinal Encoding**: It helps assign ranks to the labels/ categorical data. In this technique each category is assigned a neumerical value based on its position in the order.\n",
        "4. **Target guided Ordinal Encoding**: this technique is used to encode each categorical variable based on their relationship with the target variable. This encoding is useful when we have a categorical variable with large number of unique categories. Here, we replace categorical variable with a numerical value based on the mean and median of the target variable for that category.\n",
        "\n"
      ],
      "metadata": {
        "id": "C5VFXwP0QzJx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. What is the purpose of the design matrix in a GLM?\n"
      ],
      "metadata": {
        "id": "V4ACURnV7y4f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Design matrix is a crucial component of GLM. It is a structured representation of independent variables in GLM organised in matrix format.\n",
        "The purpose of design matrix in GLM are:\n",
        "1. Encoding Independent Variables : The design matrix represents independent variables in a structured manner where each matrix corresponds to a specific independent variable, and each row corresponds to an observation or data point. It encodes the values of the independent variables for each observation, allowing GLM to incorporate them into the model.\n",
        "2. Incorporating Nonlinear Relationships:\n",
        "The design matrix can include  interactions of the original independent variables to analyse nonlinear relationships between the predictors and the dependent variable.\n",
        "\n",
        "3. Handling Categorical Variables:\n",
        "It is important to handle categorical variables in GLM as model only understands numerical value. This is done by data encoding.\n",
        "4. Estimating Coefficients:\n",
        "The design matrix allows the GLM to estimate the coefficients for each independent variable. By incorporating the design matrix into the GLM's estimation procedure, the model determines the relationship between the independent variables and the dependent variable, estimating the magnitude and significance of the effects of each predictor.\n",
        "\n",
        "5. Making Predictions:\n",
        "Once the GLM estimates the coefficients, the design matrix is used to make predictions for new, unseen data points. By multiplying the design matrix of the new data with the estimated coefficients, the GLM can generate predictions for the dependent variable based on the values of the independent variables.\n",
        "\n"
      ],
      "metadata": {
        "id": "VqIfpi-jTkbs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. How do you test the significance of predictors in a GLM?\n"
      ],
      "metadata": {
        "id": "GK78dUXe7y1y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can test significance of predictors in GLM by following ways:\n",
        "1. Hypothesis testing\n",
        "2. Chi square test\n",
        "3. ANOVA test\n",
        "4. By estimating coefficients\n"
      ],
      "metadata": {
        "id": "rGEUFD-8UxEk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. What is the difference between Type I, Type II, and Type III sums of squares in a GLM?\n"
      ],
      "metadata": {
        "id": "a2TqqsxS7yyq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **TYPE I sums of squares**:Type I Sums of Squares, or also called Sequential Sums of Squares, assign variation to the different variables in a sequential order.It calculates the reduction in the error sum of squares by adding each predictor variable sequentially in the order specified.\n",
        "\n",
        "* TYPE II Sums of square: Type II sums of squares assess the significance of each predictor variable in the model while adjusting for the effects of other variables already in the model. It calculates the reduction in the error sum of squares by adding each predictor variable, ignoring the order of variable entry.\n",
        "* **TYPE III Sums of square:** Type III sums of squares assess the significance of each predictor variable in the model, taking into account the effects of all other variables in the model. It calculates the reduction in the error sum of squares by adding each predictor variable, adjusting for the effects of all other variables."
      ],
      "metadata": {
        "id": "stEw-dcwVowJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10. Explain the concept of deviance in a GLM."
      ],
      "metadata": {
        "id": "M9R-hDwF7yv7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a GLM, deviance is a measure of goodness of fit of the model. It quantifies the discrepency between the observed data and the predictions made by the model.\n",
        "The deviance is calculated as follows:\n",
        "\n",
        "Deviance = -2 * (log-likelihood of fitted model - log-likelihood of saturated model)"
      ],
      "metadata": {
        "id": "-kux5bD07ytX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regression:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Vi4iVS0y7yqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What is regression analysis and what is its purpose?\n"
      ],
      "metadata": {
        "id": "XEqhiYrI7ynL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regression is a kind of Supervised Machine Learning which is used when the dependent feature is having continuous value.\n",
        "\n",
        "For example : heights of students in the class.\n",
        "The purpose of regression analysis is to find the relationship between independent and dependent features where the dependent (output) variable is having continuous values."
      ],
      "metadata": {
        "id": "GmGAhZ3PX05g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What is the difference between simple linear regression and multiple linear regression?\n"
      ],
      "metadata": {
        "id": "S4OJmI2pYUKj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In **simple linear Regression**, there is only 1 independent variable which is used to estimate or predict the dependent variable. Here we can assume that the dependent variable is having a linear relationship witht he independent variable. The goal here is to find the best fit line that minimizes the difference between the observed values and predicted values based on linear relationship.\n",
        "\n",
        "In Multiple Linear Regression, There are two or more than 3 independent variables that are used to estimate or predict the dependent variable. The goal here is to find the best fit line or hyperplane that minimizes the relationship between multiple independent variables and the dependent variable."
      ],
      "metadata": {
        "id": "1IoZ0RH1YUHs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 13. How do you interpret the R-squared value in regression?"
      ],
      "metadata": {
        "id": "pkQXBoltYUFU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "R-squared is also known as coefficient of determination. It is a statistical measure that represents the proportion of variance in the dependent variable that is explained by the independent variables in a regression model.\n",
        " * R-squared= 1 - SS_res/SS_total\n",
        "* where SS_res= sum of squares of residuals\n",
        "* SS_total= sum of squares of total\n",
        " * It ranges between 0 and 1.\n",
        " * If R-square value reaches towards 1, then this is the case of overfitting.\n"
      ],
      "metadata": {
        "id": "i1AxfgtpZeRC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 14. What is the difference between correlation and regression?\n"
      ],
      "metadata": {
        "id": "j78kSlZFZeNa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Correlation** is a satistical measure that establishes the relationships between two variables.\n",
        "It quantifies how closely the values of two variables are related to each other. Correlation coefficients range from -1 to +1, where -1 indicates a perfect negative correlation, +1 indicates a perfect positive correlation, and 0 indicates no correlation.\n",
        "* **Regression**, on the other hand, is a statistical technique used to model the relationship between a dependent variable and one or more independent variables.\n",
        "Regression analysis can be used to understand the impact of specific variables on the outcome and make predictions or infer causal relationships."
      ],
      "metadata": {
        "id": "Pn2artNzZeKP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 15. What is the difference between the coefficients and the intercept in regression?"
      ],
      "metadata": {
        "id": "W79MsJ0oZeFc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Intercept:**\n",
        "The intercept, often denoted as \"b₀\" or \"β₀,\" is the value of the dependent variable when all independent variables in the model are set to zero. It represents the baseline or starting point of the regression line or surface.\n",
        "* Coefficients:\n",
        "The coefficients, also known as regression coefficients, they capture the relationship between the dependent variable and each independent variable. They indicate the direction (positive or negative) and magnitude of the effect that each independent variable has on the dependent variable.\n"
      ],
      "metadata": {
        "id": "CxiQaKj9ZeCl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 16. How do you handle outliers in regression analysis?"
      ],
      "metadata": {
        "id": "6VSFt0ECZd_0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In regression, if there are outliers then it squared error will become too high, thus effecting the performance of the model.\n",
        "Thus outliers can be handled using multiple ways:-\n",
        "1. If the points are too far, then they can be reduced.\n",
        "2. Variables can be transformed /standardized/ Normalized to reduce the outliers.\n",
        "3. In case of polynomial regression, we can take small clusters of data points to check the behaviour of outliers.\n",
        "\n"
      ],
      "metadata": {
        "id": "rJRBj1TfYUCx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 17. What is the difference between ridge regression and ordinary least squares regression?"
      ],
      "metadata": {
        "id": "vQGZF3HgwdIb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* One of the disadvantages of using Linear regression (Ordinary least squares) is overfitting.\n",
        "*  So it is essential to find solutions that help provide more regularization. One of the solutions is Ridge Regression.\n",
        "* Ridge regression is a linear model for regression like Linear regression (Ordinary least squares).\n",
        "*  But there is a difference that helps make the Ridge regression more regularized and thus avoid the problem of overfitting.\n",
        "* The ordinary least squares model seeks to find the coefficients that minimize the mean squared error.\n",
        "* On the other hand, Ridge Regression tries to find the coefficients that minimize the mean squared error and wants the magnitude of coefficients to be as small as possible. That means each feature should have a little effect on the outcome.\n",
        "* Therefore, Ridge Regression will perform worse than the ordinary least squares model on the training set. But it will give us better regularization and performance on the test set.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BBUG0luzv1AO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 18. What is heteroscedasticity in regression and how does it affect the model?"
      ],
      "metadata": {
        "id": "_3ncjJxwv08_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Heteroscedasticity in regression refers to a situation where the variability of the errors (residuals) in a regression model is not constant across different values of dependent variable. In other words, the spread of the residuals varies as the values of the independent variables change. This violates one of the assumptions of classical linear regression, known as homoscedasticity or constant variance of errors.\n",
        "It affects the model by:\n",
        "1. Giving Inaccurate Predictions,\n",
        "2. Inefficient standard errors\n",
        "3. Estimating Biased Coefficients"
      ],
      "metadata": {
        "id": "7CIg12_nv05e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 19.How do you handle multicollinearity in regression analysis?"
      ],
      "metadata": {
        "id": "vk8jAuVHxL2R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Muticollinearty in regression can be handled by:-\n",
        "1. Identifying the collinearity of features.\n",
        "2. Removing one of the correlated features.\n",
        "3. Feature Selection\n",
        "4. Reducing Dimensions of the data with the help of dimensionality reduction techniques such as PCA, Tsne etc.\n",
        "5. Feature Standardization and Feature Normalization"
      ],
      "metadata": {
        "id": "NRJ3GONRxLww"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 20. What is polynomial regression and when is it used?"
      ],
      "metadata": {
        "id": "03C-0dRDYT_2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Polynomial Regression is an extension of linear Regression that models the relationship between the independent variables and the dependent variable as a higher degree polynomial function.\n",
        "* It allows capturing non- linear relationship between the variables.\n",
        "* Polynomial regression is used in situations where the relationship between the variables is not linear and cannot be adequately represented by a straight line.\n",
        "* It is particularly useful when there is prior knowledge or a theoretical basis to believe that the relationship between the variables should be nonlinear, or when examining data patterns suggests a curvilinear relationship.\n"
      ],
      "metadata": {
        "id": "hfSDPom1YT9o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss function:"
      ],
      "metadata": {
        "id": "hvz82lS5y_XP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 21. What is a loss function and what is its purpose in machine learning?"
      ],
      "metadata": {
        "id": "nu2uq5jgy_Sf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Loss function, in machine learning is also known as error function or cost function.\n",
        "* It is a mathematical measure that quantifies the discrepency between the predicted and observed values of dependent variable.\n",
        "*  The purpose of loss function is to provide a measure of how well the model is performiing to help the learning process to minimize the error between observed values and actual values.\n"
      ],
      "metadata": {
        "id": "Bgksvl5d0tn5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 22. What is the difference between a convex and non-convex loss function?"
      ],
      "metadata": {
        "id": "RMe0Rwi3y_Pn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Convex Loss Function:**\n",
        "A convex loss function is one in which the combination of any two points on the loss function curve lies above or on the curve itself.\n",
        "\n",
        " Mathematically, a function f(x) is convex if, for any two points x₁ and x₂ in its domain and any value λ between 0 and 1, the following condition holds:\n",
        "\n",
        "f(λx₁ + (1 - λ)x₂) ≤ λf(x₁) + (1 - λ)f(x₂)\n",
        "\n",
        "In simpler terms, if you draw a straight line connecting any two points on the curve, the line will always lie above the curve.\n",
        "* **Non-Convex Loss Function:**\n",
        "A non-convex loss function does not satisfy the convexity property. In other words, there exist points on the loss function curve for which the line connecting two of those points dips below the curve.\n",
        "\n",
        "Non-convex loss functions often have multiple local minima, making it more challenging to find the optimal solution. Optimization algorithms may converge to a local minimum instead of the global minimum, resulting in a suboptimal solution."
      ],
      "metadata": {
        "id": "TeSrLOuo1Pm_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 23. What is mean squared error (MSE) and how is it calculated?"
      ],
      "metadata": {
        "id": "N2DxbPUFzzKk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mean Squared Error is a loss function used in regression problems to measure the average squared difference between the predicted values and the true values of the target variable. It quantifies the overall goodness of fit of the regression model.\n",
        "* Its advantages are:-\n",
        "It is differentiable and also has only 1 local and 1 global minima.\n",
        "* Its disadvantage is that MSE is not Robust to outliers. Also squaring the loss will square the unit of the variable as well.\n",
        "\n",
        "Mathematically, the MSE can be represented as:\n",
        "\n",
        "MSE = SSE / n\n",
        "\n",
        "where SSE is the sum of squared errors and n is the number of data points."
      ],
      "metadata": {
        "id": "UYntF8XV1lMD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 24. What is mean absolute error (MAE) and how is it calculated?"
      ],
      "metadata": {
        "id": "cWHNSNKpz05C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mean Absolute Error (MAE) is a loss function in regression analysis used to measure the average absolute difference between the predicted values and the true values of the target variable. It provides a straightforward measure of the model's average prediction error.\n",
        "* Its advantages are: It is Robust to outliers. Also the error has same unit as dependent variable.\n",
        "* Disadvantage: Convergence usually takes time and its optimization is complex.\n",
        "\n",
        "Mathematically, the MAE can be represented as:\n",
        "\n",
        "MAE = SAE / n\n",
        "\n",
        "where SAE is the sum of absolute errors and n is the number of data points."
      ],
      "metadata": {
        "id": "mRCvpKvt14Hx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 25. What is log loss (cross-entropy loss) and how is it calculated?"
      ],
      "metadata": {
        "id": "P6GTKNQJz2js"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Log loss, also known as cross-entropy loss or logarithmic loss, is a loss function used in classification tasks, especially in binary classification or multi-class classification with probabilistic models.\n",
        "\n",
        "It measures the discrepancy between predicted class probabilities and the true class labels.\n",
        "\n",
        "Mathematically, the log loss (cross-entropy loss) can be represented as:\n",
        "\n",
        "Log Loss = (-1/n) * ∑[y * log(p) + (1 - y) * log(1 - p)]\n",
        "\n",
        "where y is the true class label (0 or 1), p is the predicted probability of the true class, and the summation is performed over all data points."
      ],
      "metadata": {
        "id": "WuQnP26C2zDY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 26. How do you choose the appropriate loss function for a given problem?"
      ],
      "metadata": {
        "id": "tZ7PfoRSz4JW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Depending upon the type of problem statement and model, a suitable loss function needs to be selected from the set of available. Different parameters like type of machine learning algorithm, degrees of the percentage of outliers in the provided dataset, ease of calculating derivatives etc. play their role in choosing loss function.\n",
        "\n",
        "* Regression Loss Functions\n",
        "  1. Mean Squared Error Loss\n",
        "  2. Mean Squared Logarithmic Error Loss\n",
        "  3. Mean Absolute Error Loss\n",
        "* Binary Classification Loss Functions\n",
        "  1. Binary Cross-Entropy\n",
        "  2. Hinge Loss\n",
        "  3. Squared Hinge Loss\n",
        "* Multi-Class Classification Loss Functions\n",
        "  1. Multi-Class Cross-Entropy Loss\n",
        "  2. Sparse Multiclass Cross-Entropy Loss\n",
        "  3. Kullback Leibler Divergence Loss\n"
      ],
      "metadata": {
        "id": "86GFJ5nF2-RB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 27. Explain the concept of regularization in the context of loss functions."
      ],
      "metadata": {
        "id": "X4-r3hzhz6Cz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Regularization is a technique used to prevent the model from overfitting by adding some extra information into it.\n",
        "* It is commonly used in machine learning and statistical modelling to prevent overfitting and improve the generalization ability of the model.\n",
        "* The general form of regularized loss function is = Loss + Regularization Term.\n"
      ],
      "metadata": {
        "id": "vN_3FlEw0CXS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 28. What is Huber loss and how does it handle outliers?"
      ],
      "metadata": {
        "id": "j1AZpJR5z9UV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Huber loss is a loss function that is commonly used in regression problems to balance the effects of outliers and inliers in the data.\n",
        "\n",
        "Unlike the mean squared error (MSE) loss, which treats all errors equally, the Huber loss gives less weight to large errors, making it more robust to outliers.\n",
        "\n",
        "The Huber loss function is defined as follows:\n",
        "\n",
        "Huber Loss = { 0.5 * (y - ŷ)^2 if |y - ŷ| ≤ δ\n",
        "{ δ * |y - ŷ| - 0.5 * δ^2 if |y - ŷ| > δ\n",
        "\n",
        "where y is the true value, ŷ is the predicted value, and δ is a parameter that determines the threshold for the transition between the quadratic and linear regions of the loss function."
      ],
      "metadata": {
        "id": "Ibed_wjY4JjC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 29. What is quantile loss and when is it used?"
      ],
      "metadata": {
        "id": "NzIyl01Oz_GV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantile loss, also known as pinball loss, is a loss function used in quantile regression to measure the deviation between predicted quantiles and the true quantiles of a target variable.\n",
        "\n",
        "It is particularly useful when the goal is to estimate conditional quantiles of the response variable rather than the conditional mean.\n",
        "\n",
        "The quantile loss is defined as follows:\n",
        "\n",
        "Quantile Loss = ∑[ρ(y - ŷ)],\n",
        "\n",
        "where ρ is a function that determines the shape of the loss function, y is the true value, and ŷ is the predicted value."
      ],
      "metadata": {
        "id": "HoTriR2D4wzP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 30. What is the difference between squared loss and absolute loss?\n"
      ],
      "metadata": {
        "id": "nm8E_mjpYT62"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Key difference between Squared loss and absolute loss is:\n",
        "1. Squared Loss is not Robust to outliers whereas Absolute Loss is Robust to outliers.\n",
        "2. Squared loss value has the unit which is squared to that of the dependent variable whereas the Absolute loss value has the same unit as compared to that of dependent variable values.\n",
        "3. Squared Loss function is differentiable, we can get local and global minima using squared loss function but there is convergence in absolute loss function.\n",
        "4. In absolute loss function, optimization is complex."
      ],
      "metadata": {
        "id": "VZN20zP9YT4F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimizer (GD):"
      ],
      "metadata": {
        "id": "AW1zgkVKYT1M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 31. What is an optimizer and what is its purpose in machine learning?"
      ],
      "metadata": {
        "id": "s1-JREUlYTxo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimizer refers to an algorithm or method used to adjust the parameters of a model to minimize the loss function and improve the model's performance.\n",
        "\n",
        "The optimizer plays a crucial role in the training process by iteratively updating the model's parameters based on the gradients of the loss function.\n",
        "\n",
        "The purpose of an optimizer in machine learning is to find the optimal set of parameter values that minimize the discrepancy between the predicted values and the true values of the target variable. It aims to improve the model's ability to generalize and make accurate predictions on unseen data."
      ],
      "metadata": {
        "id": "K1e1ejEp6HZ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 32. What is Gradient Descent (GD) and how does it work?"
      ],
      "metadata": {
        "id": "IaGqzg2T6Qdv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient Descent (GD) is an optimization algorithm used to iteratively update the parameters of a model in order to minimize a given loss function. It is widely used in machine learning and deep learning for parameter estimation and model training.\n"
      ],
      "metadata": {
        "id": "h8eDDhqS6TKc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 33. What are the different variations of Gradient Descent?"
      ],
      "metadata": {
        "id": "5cRQN4Km6THn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Batch Gradient Descent (BGD) :Batch Gradient Descent calculates the gradient of the loss function using the entire training dataset in each iteration. It performs a parameter update based on the average gradient across all the training examples.\n",
        "2. Stochastic Gradient Descent (SGD):Stochastic Gradient Descent calculates the gradient using only one randomly selected data point (or a single data point from a mini-batch) in each iteration. It performs a parameter update based on the gradient of that single data point. SGD is computationally efficient as it requires the computation of the gradient for one data point at a time.\n",
        "3. Mini-Batch Gradient Descent:Mini-Batch Gradient Descent is a compromise between BGD and SGD. It calculates the gradient using a small random subset, or mini-batch, of the training data in each iteration.\n",
        "4. Momentum-Based Gradient Descent:Momentum-Based Gradient Descent algorithms introduce a momentum term that helps accelerate convergence and enhance stability."
      ],
      "metadata": {
        "id": "JKW9wt_X_GSU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 34. What is the learning rate in GD and how do you choose an appropriate value?"
      ],
      "metadata": {
        "id": "VaTKGDwr6TCK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Learning rate is a hyperparameter that controls the step size taken in each iteration during the parameter update process.  \n",
        "\n",
        "The learning rate gives you control of how big (or small) the updates are going to be. A bigger learning rate means bigger updates and, hopefully, a model that learns faster.\n",
        "It determines the magnitude of the parameter updates based on the gradient of the loss function.\n",
        "\n",
        "Choosing an appropriate learning rate is crucial for effective training and convergence.\n",
        "We can choose a learning rate by using difference values of learning rate and perform Cross validation and grid search and check the results."
      ],
      "metadata": {
        "id": "GcJEAEaX-SWz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 35. How does GD handle local optima in optimization problems?"
      ],
      "metadata": {
        "id": "iObLwJ7E6S_o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GD can handle local optima in optimization problems by:-\n",
        "1. Using regularization techniques\n",
        "2. adding noise\n",
        "3. Choosing different learning rates\n",
        "4. By choosing multiple starting points"
      ],
      "metadata": {
        "id": "7Qm6GYc291xc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 36. What is Stochastic Gradient Descent (SGD) and how does it differ from GD?"
      ],
      "metadata": {
        "id": "UQvpMp8g6S8h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stochastic Gradient Descent (SGD) is a variant of the Gradient Descent (GD) optimization algorithm used to train machine learning models.\n",
        "* It differs from GD in how it updates the model's parameters by considering only a single training example (or a small subset called a mini-batch) in each iteration, rather than the entire training dataset.\n",
        "* The main difference between stochastic gradient descent (SGD) and gradient descent (GD) is the number of data points used before each update of the parameters.\n",
        "* GD spans over the entire dataset once before each update, whereas SGD randomly takes just one data point for each update12.\n",
        "* In GD, you have to run through all the samples in your training set to do a single update for a parameter in a particular iteration, whereas in SGD, you use only one or a subset of training samples from your training set to do the update for a parameter in a particular iteration2."
      ],
      "metadata": {
        "id": "UudKEvXw9g-E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 37. Explain the concept of batch size in GD and its impact on training."
      ],
      "metadata": {
        "id": "eS-f-LRj6wQ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Gradient Descent, the batch size refers to the number of training examples used to compute the gradient and update the model's parameters in each iteration.\n",
        "* A larger batch size implies that more training examples are used to compute the gradient in each iteration. This can result in more accurate gradient estimates due to a larger sample size. However, it also requires more memory and computational resources to process the larger batch, which can slow down the training process.\n",
        "Larger batch sizes often lead to more stable updates and smoother convergence.\n",
        "\n",
        "* A smaller batch size means that fewer training examples are used in each iteration. This reduces memory requirements and speeds up computations. However, the gradient estimates may be noisier due to a smaller sample size, resulting in more variance during training."
      ],
      "metadata": {
        "id": "OzVdgGxK9Gm9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 38. What is the role of momentum in optimization algorithms?"
      ],
      "metadata": {
        "id": "NisD9liq6S54"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Momemtum in GD is used to accelerate the convergence and enhance the stability of the learning process.\n",
        "* In the Momentum-based Gradient Optimizer, a fraction of the previous update is added to the current update, which creates a momentum effect that helps the algorithm to move faster towards the minimum.\n",
        "*  Momentum enables faster convergence by adding a momentum term that accumulates the gradients over time. It increases the update step sizes for dimensions that consistently show a significant gradient, allowing the optimizer to quickly move along steep slopes towards the minimum.\n",
        "*  Higher momentum can allow for larger effective learning rates, enabling faster convergence, while lower momentum may require smaller learning rates to ensure stability."
      ],
      "metadata": {
        "id": "sRRNZpJI8hZy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 39. What is the difference between batch GD, mini-batch GD, and SGD?"
      ],
      "metadata": {
        "id": "Xj6X1fr76S27"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Batch Gradient Descent (BGD):\n",
        "* BGD calculates the gradient of the loss function using the entire training dataset in each iteration.\n",
        "* It performs a parameter update based on the average gradient across all the training examples.\n",
        "* BGD provides a precise estimate of the gradient but can be computationally expensive for large datasets.\n",
        "* BGD takes more time to converge, but the updates are more stable and less noisy compared to other variants.\n",
        "2. Mini-Batch Gradient Descent:\n",
        "* Mini-Batch Gradient Descent calculates the gradient using a randomly selected subset, or mini-batch, of the training data in each iteration.\n",
        "* The mini-batch typically contains a small number of data points, such as 10 to 1000, chosen randomly from the training set.\n",
        "* It strikes a balance between the accuracy of BGD and the computational efficiency of SGD.\n",
        "* Mini-Batch GD provides a more accurate estimate of the gradient compared to SGD due to the use of multiple data points in each iteration.\n",
        "* It is commonly used in practice as it allows for parallelization and efficient computation, especially when dealing with large datasets.\n",
        "3. Stochastic Gradient Descent (SGD):\n",
        "* SGD calculates the gradient using only one randomly selected data point (or a single data point from a mini-batch) in each iteration.\n",
        "* It performs a parameter update based on the gradient of that single data point.\n",
        "* SGD is computationally efficient and faster per iteration compared to BGD and Mini-Batch GD since it only requires the computation of the gradient for one data point at a time.\n",
        "* SGD exhibits more stochastic behavior and can be noisy during training due to the high variance in the estimated gradients.\n",
        "* Although SGD has a higher likelihood of converging to a local minimum, it can escape shallow local minima more easily compared to BGD and Mini-Batch GD."
      ],
      "metadata": {
        "id": "uxzFK9VG8N7f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 40. How does the learning rate affect the convergence of GD?"
      ],
      "metadata": {
        "id": "ezejQbNQ6Sz5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. If Learning rate is set to a large value: It can lead to divergence, where loss may increase instead of decreasing. The algorithm might fail to converge or may converge very slowly.\n",
        "2. If Learning rate is too small, the algorithm will take smaller steps, which can result in slow convergence. It may take a large number of iterations for GD to reach the minimum of the loss function.\n",
        "3. If Learning Rate is optimal, then it ccan make a balence between convergence speed and stability which will allow GD to make progress towards minimum without overshooting or converging too slowly. . It can be determined through experimentation or by using techniques like learning rate schedules or adaptive learning rate methods."
      ],
      "metadata": {
        "id": "MNQGrEPE7TAo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ".\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "I5uSBtc8YTtG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regularization:"
      ],
      "metadata": {
        "id": "ay0CkkRQ65kv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 41. What is regularization and why is it used in machine learning?"
      ],
      "metadata": {
        "id": "Bj7I-1NeBLhE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regularization is a technique used to prevent overfitting and improve the generalization performance of a model. Overfitting occurs when a model learns to fit the training data very well but fails to generalize well to unseen data.\n",
        "It introduces a penalty term  to the model's objective function, which discourages it from learning complex or intricate patterns in the training data that might not generalize well."
      ],
      "metadata": {
        "id": "6aCWhEVkBLdN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 42. What is the difference between L1 and L2 regularization?"
      ],
      "metadata": {
        "id": "AzZSL9NABuIL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* L1 regularization adds a penalty proportional to the sum of the absolute values of the model's coefficients. This encourages sparsity in the model, meaning it encourages some of the coefficients to be exactly zero, effectively performing feature selection and eliminating less important features.\n",
        "\n",
        "* L2 regularization adds a penalty proportional to the sum of the squared values of the model's coefficients. This encourages the model's coefficients to be small but non-zero, distributing the importance of features more evenly."
      ],
      "metadata": {
        "id": "2NVPkBSDBuF4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 43. Explain the concept of ridge regression and its role in regularization."
      ],
      "metadata": {
        "id": "feA1ToiQBuC-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ridge regression is a linear regression technique that incorporates L2 regularization to mitigate the issues of overfitting and improve the generalization performance of the model. It is an extension of ordinary least squares (OLS) regression.\n",
        "\n",
        "The role of ridge regression in regularization is to strike a balance between fitting the training data well and keeping the model's coefficients small. The penalty term encourages the model to find a solution where the coefficient values are spread out more evenly and prevents them from becoming too large, reducing the complexity of the model."
      ],
      "metadata": {
        "id": "Fp7cYMafBLat"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 44. What is the elastic net regularization and how does it combine L1 and L2 penalties?"
      ],
      "metadata": {
        "id": "NWRQ6ZblBLYT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Elastic Net regularization is a technique that combines both L1 (Lasso) and L2 (Ridge) regularization penalties in a linear regression model. It is designed to address the limitations of using either L1 or L2 regularization alone.\n",
        "\n",
        "In elastic net regularization, the objective function includes both the L1 and L2 penalty terms. The objective function can be represented as:\n",
        "\n",
        "Objective = Sum of squared residuals + λ₁ * (Sum of squared coefficients) + λ₂ * (Sum of absolute values of coefficients)\n",
        "\n",
        "The L1 penalty encourages sparsity in the model, promoting some coefficients to be exactly zero, thereby performing feature selection. On the other hand, the L2 penalty encourages smaller but non-zero coefficients, distributing the importance of features more evenly."
      ],
      "metadata": {
        "id": "bDS7rHnRBLVk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 45. How does regularization help prevent overfitting in machine learning models?"
      ],
      "metadata": {
        "id": "TicLP2crBLSj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Regularization helps prevent overfitting in machine learning models through the introduction of a penalty term in the model's objective function. By incorporating this penalty term, regularization discourages the model from learning overly complex or intricate patterns in the training data that may not generalize well to unseen data."
      ],
      "metadata": {
        "id": "uFy3q4C7BLPT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 46. What is early stopping and how does it relate to regularization?"
      ],
      "metadata": {
        "id": "g8dZRYzaCYGd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Early stopping is a technique used in machine learning to prevent overfitting by monitoring the performance of a model during training and stopping the training process when the model's performance on a validation set starts to deteriorate.\n",
        "\n",
        "Early stopping relates to regularization in the sense that it helps control the complexity of the model and prevents it from overfitting.\n",
        "\n",
        " Regularization techniques like L1 or L2 regularization introduce penalties to the objective function, encouraging the model to generalize better. However, these penalties alone may not be sufficient to determine the optimal point at which to stop training. Early stopping complements regularization by providing a mechanism to determine when to halt training based on the observed performance on a validation set."
      ],
      "metadata": {
        "id": "T5I3A43xCYEX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 47. Explain the concept of dropout regularization in neural networks."
      ],
      "metadata": {
        "id": "khPj3JJbCYCE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Dropout regularization is a technique used in neural networks to prevent overfitting and improve the generalization performance of the model. It involves randomly disabling or \"dropping out\" a fraction of the neurons during training.\n",
        "\n",
        "The key idea behind dropout is to introduce noise or randomness into the network by temporarily removing a subset of neurons along with their connections during each training iteration. This prevents the network from relying too heavily on any particular set of neurons and encourages the network to learn more robust and distributed representations."
      ],
      "metadata": {
        "id": "Lf8w3WmrCX-x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 48. How do you choose the regularization parameter in a model?"
      ],
      "metadata": {
        "id": "9YG4nVmBCylW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can choose the right regularization paramtere by:\n",
        "1. Grid Search\n",
        "2. Random Search\n",
        "3. Cross Validation"
      ],
      "metadata": {
        "id": "npttL1C1Cyim"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 49. What is the difference between feature selection and regularization?"
      ],
      "metadata": {
        "id": "83ePK6qvCyfj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature selection and regularization are two distinct techniques used in machine learning to improve model performance and address the issue of overfitting.\n",
        "\n",
        "\n",
        "Feature selection refers to the process of identifying and selecting a subset of relevant features from the original set of available features.\n",
        "\n",
        "Regularization, on the other hand, is a technique used during the training process of a model to prevent overfitting and improve generalization.\n",
        "\n",
        "Feature selection focuses on choosing a subset of relevant features, while regularization aims to control the complexity of the model during training.\n"
      ],
      "metadata": {
        "id": "tVRWS2ZVDDJp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 50. What is the trade-off between bias and variance in regularized models?\n"
      ],
      "metadata": {
        "id": "1tDlREAx6-mV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bias Variance Tradeoff refers to the final goal of our ML model which is to get a low generalization error.\n",
        "i.e. Low Bias and Low Variance.\n",
        "\n",
        "Using regularization , Increasing the regularization parameter (λ) leads to an increase in bias and a decrease in variance.\n",
        "\n",
        "Decreasing the regularization parameter (λ) leads to a decrease in bias and an increase in variance.\n",
        "\n",
        "The optimal regularization parameter strikes a balance between bias and variance to achieve the best trade-off.\n",
        "To acheive this techniques such as Grid Search or Cross Validation are used.\n"
      ],
      "metadata": {
        "id": "g529c9mEDSSA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SVM:\n",
        "\n"
      ],
      "metadata": {
        "id": "PHi1ptTdbx3q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 51. What is Support Vector Machines (SVM) and how does it work?"
      ],
      "metadata": {
        "id": "tsOmmnGQbx0c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Support Vector Machines (SVM) is a powerful supervised machine learning algorithm used for classification and regression tasks. It's particularly effective for solving binary classification problems, where the goal is to separate data points into two classes.\n",
        "It finds a hyperplane that maximally seprates the data points of different classes.\n",
        "It can be used to handle high dimensional data."
      ],
      "metadata": {
        "id": "7r07L3Y5cS6i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 52. How does the kernel trick work in SVM?"
      ],
      "metadata": {
        "id": "ziphFxgSbxtc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The kernel trick is a crucial component of Support Vector Machines (SVM) that allows them to efficiently operate in high-dimensional feature spaces without explicitly computing the coordinates of the data points in that space. It provides a way to implicitly represent the data in a higher-dimensional space, even when the original data is in a lower-dimensional space. This technique makes SVM highly flexible and effective in dealing with complex data."
      ],
      "metadata": {
        "id": "Pl5886Mecss2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 53. What are support vectors in SVM and why are they important?"
      ],
      "metadata": {
        "id": "4f0zE0Tfbxq2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Support vectors play a critical role in Support Vector Machines (SVM) and are essential to the algorithm's effectiveness and interpretability. In SVM, support vectors are the data points from the training set that lie closest to the decision boundary (hyperplane) and have the most influence on its position.\n",
        "They are important because of the following reasons:\n",
        "1.  Support vectors are crucial because they define the margin and help establish the separation between classes.\n",
        "2. SVM is a sparse algorithm, meaning that the decision boundary depends only on a small subset of the training data, namely the support vectors."
      ],
      "metadata": {
        "id": "9AxKxh8tcwQ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 54. Explain the concept of the margin in SVM and its impact on model performance."
      ],
      "metadata": {
        "id": "p5cOwoWGbxoA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The margin in Support Vector Machines (SVM) is a key concept that plays a vital role in determining the optimal decision boundary and has a significant impact on the model's performance.\n",
        "\n",
        " The margin provides a measure of how well the data points of different classes are separated.\n",
        "\n",
        " The margin also contributes to the robustness of SVM against noisy data and outliers.\n",
        "\n",
        "  SVM aims to achieve good generalization performance, which refers to the ability to accurately classify unseen examples.\n"
      ],
      "metadata": {
        "id": "Y3vPDJ3UdKNX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 55. How do you handle unbalanced datasets in SVM?"
      ],
      "metadata": {
        "id": "fBQdb5FEbxks"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unbalenced datasets in SVM are handles using:\n",
        "1. Upsampling\n",
        "2. Downsampling\n",
        "3. Providing weights to the classes based on their importane.\n"
      ],
      "metadata": {
        "id": "vWkpgfBBdenY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 56. What is the difference between linear SVM and non-linear SVM?\n"
      ],
      "metadata": {
        "id": "WtjdMziSbHYP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Linear SVM**: Linear SVM is designed for datasets where the classes can be separated by a linear decision boundary. It constructs a linear hyperplane that maximizes the margin between the classes. The decision boundary is a straight line (in two dimensions) or a hyperplane (in higher dimensions) that separates the classes. Linear SVM is computationally efficient and works well when the data is linearly separable. However, it may struggle with datasets that have complex or nonlinear relationships between the features.\n",
        "\n",
        "Non-linear SVM: Non-linear SVM, on the other hand, can handle datasets that are not linearly separable by transforming the data into a higher-dimensional feature space. Non-linear SVM achieves this by using kernel functions, such as the polynomial kernel or the Gaussian (RBF) kernel, to implicitly map the data points into a higher-dimensional space where they can be linearly separable. In the transformed feature space, linear SVM is applied to find a nonlinear decision boundary that separates the classes. Non-linear SVM allows for more flexible decision boundaries that can capture complex relationships in the data."
      ],
      "metadata": {
        "id": "ssBI-X3bd1Qy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 57. What is the role of C-parameter in SVM and how does it affect the decision boundary?"
      ],
      "metadata": {
        "id": "fRxXM_RybHUx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The C-parameter in Support Vector Machines (SVM) is a regularization parameter that controls the trade-off between achieving a wider margin and allowing for margin violations or misclassifications. It influences the flexibility of the decision boundary and can significantly impact the model's performance.\n",
        "It affects the decision boundary by:\n",
        "1. Reducing margin and classification errors.\n",
        "2. Using soft margin classification to make balence between margin and errors.\n",
        "3. Handling imbalenced data\n",
        "4. Making a tradeoff between margin and errors , thus reducing misclassifications."
      ],
      "metadata": {
        "id": "j1EhmN7-d8cZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 58. Explain the concept of slack variables in SVM."
      ],
      "metadata": {
        "id": "hyM5RqYGcEtp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Slack variables are introduced in SVM to handle cases where the data is not linearly separable by the given hyperplane. They allow for margin violations and misclassifications, providing flexibility in handling complex datasets. The regularization parameter C determines the balance between maximizing the margin and controlling the errors. Soft margin classification with slack variables enables SVM to handle non-linearly separable data and achieve a trade-off between accuracy and margin size."
      ],
      "metadata": {
        "id": "awo0sqbjeqgB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 59. What is the difference between hard margin and soft margin in SVM?"
      ],
      "metadata": {
        "id": "MHlYrazrcEqq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hard Margin SVM:**\n",
        "Hard margin SVM is designed for datasets that are linearly separable, meaning that it is possible to find a hyperplane that perfectly separates the data points of different classes. The objective of hard margin SVM is to find the hyperplane with the maximum margin between the classes while ensuring that all data points are correctly classified.\n",
        "\n",
        "Soft Margin SVM:\n",
        "Soft margin SVM is an extension of hard margin SVM that can handle datasets that are not linearly separable. It introduces the concept of slack variables to allow for margin violations or misclassifications within certain limits. Soft margin SVM provides a more flexible approach by allowing some errors to achieve a better balance between maximizing the margin and controlling errors."
      ],
      "metadata": {
        "id": "hkWE-0Jje6Gt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 60. How do you interpret the coefficients in an SVM model?"
      ],
      "metadata": {
        "id": "wrT_rn2PcEn5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Coefficients in SVM model can be interpreted by:-\n",
        "\n",
        "**In linear SVM:-**\n",
        "* A positive coefficient indicates that an increase in the corresponding feature's value pushes the decision boundary toward the positive class.\n",
        "* A negative coefficient indicates that an increase in the corresponding feature's value pushes the decision boundary toward the negative class.\n",
        "* The magnitude of the coefficients reflects the importance of the corresponding features in separating the classes.\n",
        "\n",
        "**In Non-Linear SVM with kernels:**\n",
        "* Support vectors are the data points closest to the decision boundary and play a key role in defining the decision boundary.\n",
        "* Each support vector has an associated weight or dual coefficient. The magnitude of these weights indicates the importance of the corresponding support vectors in defining the decision boundary.\n",
        "* Each support vector has an associated weight or dual coefficient. The magnitude of these weights indicates the importance of the corresponding support vectors in defining the decision boundary.\n",
        "\n"
      ],
      "metadata": {
        "id": "UXRYOMGeb9c9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision Trees:"
      ],
      "metadata": {
        "id": "9K5RJ0MKfBIn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 61. What is a decision tree and how does it work?"
      ],
      "metadata": {
        "id": "rkeADow_cEku"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision Trees are versatile Machine Learning algorithms that can perform both\n",
        "classification and regression tasks, and even multioutput tasks. They are very powerful algorithms,capable of fitting complex datasets.\n",
        " It is a flowchart-like structure where each internal node represents a feature or attribute, each branch represents a decision or rule, and each leaf node represents an outcome or a class label.\n",
        "\n",
        " The construction of a decision tree involves a process called recursive partitioning. Here's a general overview of how it works:\n",
        "\n",
        "Data preparation: The decision tree algorithm starts with a labeled dataset, where each data instance has a set of features and a corresponding class label or target variable.\n",
        "\n",
        "Feature selection: The algorithm analyzes the dataset to determine the most informative features for making decisions. It evaluates the features based on criteria such as information gain, Gini impurity, or entropy.\n",
        "\n",
        "Root node selection: The algorithm chooses the best feature from the available features as the root node of the tree. This feature is selected based on its ability to maximize the separation or classification of the data.\n",
        "\n",
        "Splitting: The selected feature is used to partition the dataset into subsets based on its possible attribute values. Each subset corresponds to a branch originating from the root node, representing a different decision or rule.\n",
        "\n",
        "Recursion: The above steps are then repeated recursively for each subset or branch of the tree, treating them as separate datasets. This process continues until a termination condition is met. The termination condition could be reaching a maximum depth, a minimum number of samples, or a purity threshold.\n",
        "\n",
        "Leaf node assignment: Once the recursive partitioning is complete, the leaf nodes are assigned class labels or target values based on the majority class or the average value of the instances in each leaf.\n",
        "\n",
        "Pruning (optional): After constructing the initial decision tree, a pruning step can be performed to simplify and reduce overfitting. Pruning involves removing unnecessary branches or merging similar leaf nodes.\n",
        "\n",
        "Prediction: To make predictions on new, unseen instances, the decision tree traverses the tree from the root node, following the decision rules at each internal node until it reaches a leaf node. The class label or target value associated with that leaf node is then assigned as the predicted outcome.\n"
      ],
      "metadata": {
        "id": "JoOt-Z4pV_de"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 62. How do you make splits in a decision tree?"
      ],
      "metadata": {
        "id": "acZv6HXRV_bV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can make splits in decision tree by:-\n",
        "1. Feature Selection\n",
        "2. split selection\n",
        "3. Binary Splits\n",
        "4. recursive splitting\n",
        "5. Continuous target variable"
      ],
      "metadata": {
        "id": "du3vhyYiV_YX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 63. What are impurity measures (e.g., Gini index, entropy) and how are they used in decision trees?"
      ],
      "metadata": {
        "id": "JX-vrX6mV_UU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gini index**: The Gini index measures the probability of misclassifying a randomly selected instance from a subset. It ranges from 0 to 1, where 0 indicates a pure subset (all instances belong to the same class) and 1 indicates maximum impurity (an equal distribution of instances across all classes).\n",
        "\n",
        "**Entropy**: Entropy is a measure of the average amount of information needed to identify the class of an instance in a subset. It also ranges from 0 to 1, with 0 representing a pure subset and 1 representing maximum impurity.\n",
        "\n",
        "During the construction of a decision tree, the algorithm considers various candidate splits and evaluates their impurity measures. The feature and threshold that yield the lowest impurity (or highest information gain) are chosen as the optimal split. This process is repeated recursively for each subsequent node in the tree, resulting in a tree structure that partitions the data into homogeneous subsets based on class distributions."
      ],
      "metadata": {
        "id": "V3ZOrDtEV_RE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 64. Explain the concept of information gain in decision trees."
      ],
      "metadata": {
        "id": "leuEers5V_Lf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Information gain is a concept used in decision trees to measure the reduction in entropy or impurity achieved by splitting the data based on a particular feature. It quantifies the amount of information gained or the decrease in uncertainty about the target variable after the split."
      ],
      "metadata": {
        "id": "7wYVT7MOXBYw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 65. How do you handle missing values in decision trees?"
      ],
      "metadata": {
        "id": "hksfn0rEXBWS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Missing values can be handled in decision trees by:-\n",
        "1. Imputation of mean, median , mode or predicted values.\n",
        "2. Deleting the rows containing missing values.\n",
        "3. deleing the column containing most missing values.\n",
        "4. Considering missing values as a seprate category.\n"
      ],
      "metadata": {
        "id": "n9S1kz6WXBS8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 66. What is pruning in decision trees and why is it important?"
      ],
      "metadata": {
        "id": "_CTgQ9x6Xjcp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pruning in decision trees is the process of reducing the size of a fully grown tree by removing or collapsing unnecessary branches or leaf nodes. It is an important step to prevent overfitting and improve the generalization ability of the decision tree model."
      ],
      "metadata": {
        "id": "SVzKzBqaXlsY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 67. What is the difference between a classification tree and a regression tree?"
      ],
      "metadata": {
        "id": "6lq1nshzXlpv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Classification Tree**: A classification tree is used for predicting categorical or discrete class labels. It is commonly employed in classification tasks where the target variable is qualitative or belongs to a limited set of predefined classes. The goal of a classification tree is to learn decision rules that can accurately assign instances to specific classes based on the values of their features. Each leaf node in a classification tree represents a class label, and the path from the root to a leaf node represents the decision rules to reach that class.\n",
        "\n",
        "**Regression Tree:** A regression tree, on the other hand, is used for predicting continuous or numerical target variables. It is typically employed in regression tasks where the target variable is quantitative and represents a range of values. The objective of a regression tree is to learn decision rules that can estimate the numerical value of the target variable based on the feature values of an instance. The leaf nodes in a regression tree contain predicted values, and the path from the root to a leaf node represents the decision rules to compute that prediction\n"
      ],
      "metadata": {
        "id": "1pIqJvupXlnN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 68. How do you interpret the decision boundaries in a decision tree?"
      ],
      "metadata": {
        "id": "p2gU1YXkXlkB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Boundries can be interpreted in decision trees by:-\n",
        "1. Splitting data in herrarchical structure.\n",
        "2. Using Recursive partitioning based on a selected feature and treshold.\n",
        "3. Visualizing the feature space and class labels.\n"
      ],
      "metadata": {
        "id": "hVyO5U4aX5A2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 69. What is the role of feature importance in decision trees?"
      ],
      "metadata": {
        "id": "VKLDwRUeX4wO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Feature importance in decision trees refers to the measure of the significance or contribution of each feature in the tree's decision-making process.\n",
        "* Understanding feature importance is valuable for various reasons, including feature selection, understanding the data, and model interpretation.\n",
        "* Feature importance provides insights into the relative importance of different features in making predictions or classifications.\n",
        "* Feature importance aids in explaining and interpreting the decision-making process of the model.\n",
        "* Feature importance can be used to compare the relative contributions of features across different models or algorithms.\n"
      ],
      "metadata": {
        "id": "fLyK5iQgYTxG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### 70. What are ensemble techniques and how are they related to decision trees?\n"
      ],
      "metadata": {
        "id": "VtgKY6P57Ab8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensemble techniques are machine learning methods that combine the predictions of multiple individual models to improve overall performance and generalization. They leverage the diversity and collective wisdom of multiple models to make more accurate and robust predictions.\n",
        "\n",
        " Decision trees are often used as the base or constituent models within ensemble techniques."
      ],
      "metadata": {
        "id": "Jqf3ICa4YnLr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ensemble Techniques:"
      ],
      "metadata": {
        "id": "X9fUbwLZYlAd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 71. What are ensemble techniques in machine learning?"
      ],
      "metadata": {
        "id": "TgpX1STSYyMI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some common ensemble techniques in machine learning are:-\n",
        "1. Bagging (bootstrap Aggregation)\n",
        "2. Boosting\n",
        "3. Random Forest\n",
        "4. Stacking\n"
      ],
      "metadata": {
        "id": "t7YvuQ4DYyKF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 72. What is bagging and how is it used in ensemble learning?"
      ],
      "metadata": {
        "id": "tsWpsT_AYyHg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bagging, short for Bootstrap Aggregating, is an ensemble learning technique that involves combining the predictions of multiple base models to form a more robust and accurate prediction.\n",
        "\n",
        "Bagging, in machine learning is commonly used to reduce variance, increase stability, and improve the generalization performance of machine learning models.\n",
        "* It reduces variance\n",
        "* It increases stability of machine learning model.\n",
        "*  It improoves the generalization performance of the ensemble model.\n"
      ],
      "metadata": {
        "id": "ulN44flHYyEj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 73. Explain the concept of bootstrapping in bagging."
      ],
      "metadata": {
        "id": "9bw0wGn9ZHPh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In bagging (Bootstrap Aggregating), bootstrapping is a key concept used to create multiple diverse training datasets from the original training data. Bootstrapping involves random sampling with replacement, where new datasets are generated by selecting instances from the original dataset."
      ],
      "metadata": {
        "id": "9DanFbZ5ZU8W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 74. What is boosting and how does it work?"
      ],
      "metadata": {
        "id": "9uIC5YwzZqSt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Boosting is an ensemble learning technique that combines multiple weak or base models to create a strong and highly accurate predictive model. Unlike bagging, which focuses on reducing variance, boosting aims to reduce bias and improve the overall model performance by sequentially training models that correct the mistakes of their predecessors.\n",
        "* Boosting starts by training an initial weak learner or base model on the original training dataset. The weak model can be any machine learning algorithm that performs slightly better than random guessing.\n",
        "* Each instance in the training dataset is associated with a weight. Initially, all weights are set equally, so each instance has the same importance during training.\n",
        "* Boosting proceeds through a series of iterations or rounds. In each round, a new weak model is trained on the modified training dataset, where the weights of the instances have been adjusted.\n",
        "* After each iteration, the newly trained weak model is combined with the existing ensemble. However, each model's contribution to the final prediction is weighted based on its performance on the training data.\n",
        "* The boosting process continues until a predefined stopping criterion is met.\n",
        "*  The final prediction is obtained by aggregating the weighted predictions of all the weak models in the ensemble."
      ],
      "metadata": {
        "id": "uzEVO7RxZU5e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 75. What is the difference between AdaBoost and Gradient Boosting?"
      ],
      "metadata": {
        "id": "Dtqu-CzJZ_Pz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* AdaBoost (Adaptive Boosting) and Gradient Boosting are both popular boosting algorithms used in ensemble learning\n",
        "* In AdaBoost, the weight of each training instance is adjusted based on its misclassification error.  In Gradient Boosting, the weights are not adjusted. Instead, the subsequent weak models are trained to minimize the residual errors.\n",
        "*  AdaBoost typically uses weak models with a high bias and low variance, such as decision stumps (simple decision trees with only one split). Gradient Boosting uses decision trees as weak models.\n",
        "* AdaBoost uses a learning rate parameter to control the contribution of each weak model to the final prediction.Gradient Boosting also incorporates a learning rate parameter, but it is typically used to shrink the contribution of each weak model to reduce overfitting.\n",
        "* AdaBoost optimizes the exponential loss function, which emphasizes correcting the misclassified instances.Gradient Boosting can optimize different loss functions depending on the specific task, such as mean squared error (MSE) for regression or log loss for classification.\n",
        "* AdaBoost combines the predictions of all weak models through weighted voting, where the weights are based on the accuracy of each weak model.Gradient Boosting combines the predictions of all weak models by summing them together."
      ],
      "metadata": {
        "id": "indn486MaAej"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 76. What is the purpose of random forests in ensemble learning?"
      ],
      "metadata": {
        "id": "Sqrx43lTaAWL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Random Forest is an ensemble learning method that utilizes the concept of bagging and decision trees to create a robust and highly accurate predictive model. The purpose of Random Forest is to leverage the diversity and collective knowledge of multiple decision trees to improve prediction performance and handle complex datasets.\n",
        "\n",
        "Random Forest is widely used in various machine learning tasks, including classification and regression, due to its effectiveness in handling complex datasets, reducing variance, and providing robust predictions. It is a versatile ensemble method that offers advantages in terms of accuracy, interpretability (through feature importance), and computational efficiency."
      ],
      "metadata": {
        "id": "ZJUKNFAwabfj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 77. How do random forests handle feature importance?"
      ],
      "metadata": {
        "id": "-J3T7Qngabc9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Features in random forests are handled by:-\n",
        "1. The Gini index measures the probability of misclassifying a randomly chosen instance in a given node. The greater the impurity reduction achieved by a feature, the more important it is considered to be.\n",
        "2. The importance of a feature is calculated by averaging or summing the Gini importance values across all trees, providing an overall measure of importance for that feature.\n",
        "3.  Normalization ensures that the importance values are on a comparable scale and allows for easier interpretation and comparison among features.\n",
        "4.  Features with higher importance scores are considered more influential in the Random Forest's prediction process."
      ],
      "metadata": {
        "id": "71DFXbgJabZ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 78. What is stacking in ensemble learning and how does it work?"
      ],
      "metadata": {
        "id": "SJNmIkg5a0HZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Stacking, also known as stacked generalization, is an ensemble learning technique that combines the predictions of multiple individual models (base models) by training a meta-model that learns to combine their predictions. Stacking aims to improve prediction accuracy and capture diverse patterns in the data.\n",
        "* The stacking process begins by training a set of diverse base models on the original training data. These base models can be different machine learning algorithms or variations of the same algorithm with different hyperparameters.\n",
        "* After training the base models, a holdout dataset is created from the original training data.\n",
        "* The holdout dataset is then used to obtain predictions from each of the trained base models.\n",
        "* The base models' predictions are treated as additional input features for the meta-model.\n",
        "*  Once the meta-model is trained, it can be used to make predictions on new, unseen data. When a new instance is presented to the stacking ensemble for prediction, each base model generates its prediction, and these predictions are then combined using the trained meta-model.\n"
      ],
      "metadata": {
        "id": "3fohibgqa2d2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 79. What are the advantages and disadvantages of ensemble techniques?"
      ],
      "metadata": {
        "id": "Qp_svu_LbFta"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Advantages:-\n",
        "1. By combining the predictions of multiple models, ensemble methods can mitigate the biases and errors of individual models, leading to more robust and accurate predictions.\n",
        "2.  The ensemble can reduce the impact of outliers or noise in the data, resulting in more reliable predictions.\n",
        "3.  Ensemble methods excel in handling complex relationships and patterns in the data.\n",
        "4. Ensembles are generally more robust than individual models, as they can reduce overfitting and handle data with high variance or uncertainty.\n",
        "5. Some ensemble techniques, such as Random Forests, provide measures of feature importance.\n",
        "\n",
        "Disadvantages-\n",
        "1. Ensemble techniques typically require training multiple models, which can be computationally expensive and time-consuming.\n",
        "2. Ensembles can be more complex and difficult to interpret than individual models.\n",
        "3. While ensemble techniques can reduce overfitting compared to individual models, there is still a risk of overfitting if the ensemble becomes too complex or is not properly regularized.\n",
        "4. Ensemble techniques often have hyperparameters that need to be tuned to achieve optimal performance.\n",
        "5. It also increases model complexity."
      ],
      "metadata": {
        "id": "ui-ugwQWbH6j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### 80. How do you choose the optimal number of models in an ensemble?\n",
        "\n"
      ],
      "metadata": {
        "id": "BYnXM6dV7AY-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. By Cross Validation\n",
        "2. By Analysing Learning curve\n",
        "3. By considering available computational resources and the time required to run.\n"
      ],
      "metadata": {
        "id": "aerhD7ob7AV-"
      }
    }
  ]
}